{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"clf = CountVectorizer()\nX_train_cv =  clf.fit_transform(X_train)\nX_test_cv = clf.transform(X_test)\ntf_transformer = TfidfTransformer(use_idf=True).fit(X_train_cv)\nX_train_tf = tf_transformer.transform(X_train_cv)\nX_test_tf = tf_transformer.transform(X_test_cv)\nnb_clf = MultinomialNB()\nMultinomialNB()\nnb_clf.fit(X_train_tf, y_train)\nnb_pred = nb_clf.predict(X_test_tf)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define LSTM Model\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=True))\nmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(2, activation='sigmoid'))\n#  Compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# Convert the labels to one-hot encoding\ny_train = pd.get_dummies(y_train).values\ny_test = pd.get_dummies(y_test).values\n# Train the model\nfrom keras.callbacks import EarlyStopping\nhistory = model.fit(X_train, y_train, epochs=10, batch_size=1024 , validation_split=0.1,\n                   verbose=1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n\n# Evaluate the model on the test data\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Test Accuracy: {:.4f}\".format(accuracy))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define important libraries for CNN Model\nfrom keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n# Define the CNN Model\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=True))\nmodel.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(2, activation='sigmoid'))\n# Compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# Convert the labels to one-hot encoding\ny_train = pd.get_dummies(y_train).values\ny_test = pd.get_dummies(y_test).values\n# Train the model\nfrom keras.callbacks import EarlyStopping\nhistory = model.fit(X_train, y_train, epochs=10, batch_size=1024 , validation_split=0.1,\n                   verbose=1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n\n# Evaluate the model on the test data\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Test Accuracy: {:.4f}\".format(accuracy))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom transformers import BertTokenizer\nfrom torch.utils.data import TensorDataset\nfrom transformers import DistilBertTokenizerFast\n\n\n\nMAX_LEN = 128\n# Load the tokenizer\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\n# Tokenize the sentences and convert them to numerical input\ninput_ids = []\nattention_masks = []\n\n# For every sentence in the dataset\nfor sent in data['review']:\n    # Encode the sentence using the tokenizer\n    encoded_sent = tokenizer.encode_plus(\n        text=sent,  # Sentence to encode\n        add_special_tokens = True,  # Add [CLS] and [SEP]\n        max_length = MAX_LEN,  # Truncate longer sentences\n        pad_to_max_length = True,  # Pad shorter sentences\n        return_attention_mask = True,  # Generate attention mask\n        return_tensors='pt'  # Return PyTorch tensors\n    )\n    \n    # Add the numerical input and attention mask to the lists\n    input_ids.append(encoded_sent['input_ids'])\n    attention_masks.append(encoded_sent['attention_mask'])\n    \n# Load the pre-trained model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\",\n    num_labels=2,  # Number of output labels\n    output_attentions=False,\n    output_hidden_states=False,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs = 4,\n    per_device_train_batch_size = 128,\n    per_device_eval_batch_size = 128,\n    warmup_steps=0,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    evaluation_strategy='epoch',\n    save_total_limit=1,\n    learning_rate=2e-5,\n)\n\n# Assuming you have defined model, eval_dataset, and args\ndef compute_metrics(eval_prediction_output, labels=None):\n    logits = eval_prediction_output.predictions\n    preds = logits.argmax(axis=-1)\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds, average='weighted')\n    return {'accuracy': acc, 'f1': f1}\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=torch.utils.data.TensorDataset(train_inputs, train_masks, train_labels),\n    eval_dataset=torch.utils.data.TensorDataset(test_inputs, test_masks, test_labels),\n    data_collator=lambda data: {'input_ids': torch.stack([item[0] for item in data]),\n                                'attention_mask': torch.stack([item[1] for item in data]),\n                                'labels': torch.stack([item[2] for item in data])},\n    compute_metrics=lambda eval_prediction_output: compute_metrics(eval_prediction_output, test_labels)\n)\n\n# Train the model\ntrainer.train()\n\n# Evaluate the model\ntrainer.evaluate()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndef predict(text):\n    text = clean_text(text)\n    encoded_review = tokenizer.encode_plus(text, max_length=MAX_LEN,\n    add_special_tokens=True,\n    return_token_type_ids=False,\n    pad_to_max_length=True, #True,\n    truncation='longest_first',\n    return_attention_mask=True,\n    return_tensors='pt'\n    )\n    input_ids = encoded_review['input_ids'].to(device) #(input_ids + ([tokenizer.pad_token_id] * padding_length)).to(device)  \n    attention_mask = encoded_review['attention_mask'].to(device)\n    \n    output = model(input_ids, attention_mask)\n    _, prediction = torch.max(output[0], dim=1)\n    return prediction[0].cpu().detach().numpy()","metadata":{},"execution_count":null,"outputs":[]}]}