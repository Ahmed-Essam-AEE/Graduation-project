{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "301f91fb",
   "metadata": {},
   "source": [
    "# Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9c00149",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mido_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Text Preprocessing\n",
    "import re, string\n",
    "import nltk\n",
    "import scapy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "import pyarabic.araby as araby\n",
    "from textblob import TextBlob\n",
    "# Pytorch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# Transformers\n",
    "import transformers\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
    "from transformers import Trainer , TrainingArguments\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "#random\n",
    "import random\n",
    "seed_value=42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "#os and warnings\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "\n",
    "Model_Used = \"UBC-NLP/MARBERT\"\n",
    "Task_Name = \"classification\"\n",
    "\n",
    "import pickle\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d96aa",
   "metadata": {},
   "source": [
    "# Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a83296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"arabic\"))\n",
    "stop_word_comp = {\"،\",\"آض\",\"آمينَ\",\"آه\",\"آهاً\",\"آي\",\"أ\",\"أب\",\"أجل\",\"أجمع\",\"أخ\",\"أخذ\",\"أصبح\",\"أضحى\",\"أقبل\",\"أقل\",\"أكثر\",\"ألا\",\"أم\",\"أما\",\"أمامك\",\"أمامكَ\",\"أمسى\",\"أمّا\",\"أن\",\"أنا\",\"أنت\",\"أنتم\",\"أنتما\",\"أنتن\",\"أنتِ\",\"أنشأ\",\"أنّى\",\"أو\",\"أوشك\",\"أولئك\",\"أولئكم\",\"أولاء\",\"أولالك\",\"أوّهْ\",\"أي\",\"أيا\",\"أين\",\"أينما\",\"أيّ\",\"أَنَّ\",\"أََيُّ\",\"أُفٍّ\",\"إذ\",\"إذا\",\"إذاً\",\"إذما\",\"إذن\",\"إلى\",\"إليكم\",\"إليكما\",\"إليكنّ\",\"إليكَ\",\"إلَيْكَ\",\"إلّا\",\"إمّا\",\"إن\",\"إنّما\",\"إي\",\"إياك\",\"إياكم\",\"إياكما\",\"إياكن\",\"إيانا\",\"إياه\",\"إياها\",\"إياهم\",\"إياهما\",\"إياهن\",\"إياي\",\"إيهٍ\",\"إِنَّ\",\"ا\",\"ابتدأ\",\"اثر\",\"اجل\",\"احد\",\"اخرى\",\"اخلولق\",\"اذا\",\"اربعة\",\"ارتدّ\",\"استحال\",\"اطار\",\"اعادة\",\"اعلنت\",\"اف\",\"اكثر\",\"اكد\",\"الألاء\",\"الألى\",\"الا\",\"الاخيرة\",\"الان\",\"الاول\",\"الاولى\",\"التى\",\"التي\",\"الثاني\",\"الثانية\",\"الذاتي\",\"الذى\",\"الذي\",\"الذين\",\"السابق\",\"الف\",\"اللائي\",\"اللاتي\",\"اللتان\",\"اللتيا\",\"اللتين\",\"اللذان\",\"اللذين\",\"اللواتي\",\"الماضي\",\"المقبل\",\"الوقت\",\"الى\",\"اليوم\",\"اما\",\"امام\",\"امس\",\"ان\",\"انبرى\",\"انقلب\",\"انه\",\"انها\",\"او\",\"اول\",\"اي\",\"ايار\",\"ايام\",\"ايضا\",\"ب\",\"بات\",\"باسم\",\"بان\",\"بخٍ\",\"برس\",\"بسبب\",\"بسّ\",\"بشكل\",\"بضع\",\"بطآن\",\"بعد\",\"بعض\",\"بك\",\"بكم\",\"بكما\",\"بكن\",\"بل\",\"بلى\",\"بما\",\"بماذا\",\"بمن\",\"بن\",\"بنا\",\"به\",\"بها\",\"بي\",\"بيد\",\"بين\",\"بَسْ\",\"بَلْهَ\",\"بِئْسَ\",\"تانِ\",\"تانِك\",\"تبدّل\",\"تجاه\",\"تحوّل\",\"تلقاء\",\"تلك\",\"تلكم\",\"تلكما\",\"تم\",\"تينك\",\"تَيْنِ\",\"تِه\",\"تِي\",\"ثلاثة\",\"ثم\",\"ثمّ\",\"ثمّة\",\"ثُمَّ\",\"جعل\",\"جلل\",\"جميع\",\"جير\",\"حار\",\"حاشا\",\"حاليا\",\"حاي\",\"حتى\",\"حرى\",\"حسب\",\"حم\",\"حوالى\",\"حول\",\"حيث\",\"حيثما\",\"حين\",\"حيَّ\",\"حَبَّذَا\",\"حَتَّى\",\"حَذارِ\",\"خلا\",\"خلال\",\"دون\",\"دونك\",\"ذا\",\"ذات\",\"ذاك\",\"ذانك\",\"ذانِ\",\"ذلك\",\"ذلكم\",\"ذلكما\",\"ذلكن\",\"ذو\",\"ذوا\",\"ذواتا\",\"ذواتي\",\"ذيت\",\"ذينك\",\"ذَيْنِ\",\"ذِه\",\"ذِي\",\"راح\",\"رجع\",\"رويدك\",\"ريث\",\"رُبَّ\",\"زيارة\",\"سبحان\",\"سرعان\",\"سنة\",\"سنوات\",\"سوف\",\"سوى\",\"سَاءَ\",\"سَاءَمَا\",\"شبه\",\"شخصا\",\"شرع\",\"شَتَّانَ\",\"صار\",\"صباح\",\"صفر\",\"صهٍ\",\"صهْ\",\"ضد\",\"ضمن\",\"طاق\",\"طالما\",\"طفق\",\"طَق\",\"ظلّ\",\"عاد\",\"عام\",\"عاما\",\"عامة\",\"عدا\",\"عدة\",\"عدد\",\"عدم\",\"عسى\",\"عشر\",\"عشرة\",\"علق\",\"على\",\"عليك\",\"عليه\",\"عليها\",\"علًّ\",\"عن\",\"عند\",\"عندما\",\"عوض\",\"عين\",\"عَدَسْ\",\"عَمَّا\",\"غدا\",\"غير\",\"ـ\",\"ف\",\"فان\",\"فلان\",\"فو\",\"فى\",\"في\",\"فيم\",\"فيما\",\"فيه\",\"فيها\",\"قال\",\"قام\",\"قبل\",\"قد\",\"قطّ\",\"قلما\",\"قوة\",\"كأنّما\",\"كأين\",\"كأيّ\",\"كأيّن\",\"كاد\",\"كان\",\"كانت\",\"كذا\",\"كذلك\",\"كرب\",\"كل\",\"كلا\",\"كلاهما\",\"كلتا\",\"كلم\",\"كليكما\",\"كليهما\",\"كلّما\",\"كلَّا\",\"كم\",\"كما\",\"كي\",\"كيت\",\"كيف\",\"كيفما\",\"كَأَنَّ\",\"كِخ\",\"لئن\",\"لا\",\"لات\",\"لاسيما\",\"لدن\",\"لدى\",\"لعمر\",\"لقاء\",\"لك\",\"لكم\",\"لكما\",\"لكن\",\"لكنَّما\",\"لكي\",\"لكيلا\",\"للامم\",\"لم\",\"لما\",\"لمّا\",\"لن\",\"لنا\",\"له\",\"لها\",\"لو\",\"لوكالة\",\"لولا\",\"لوما\",\"لي\",\"لَسْتَ\",\"لَسْتُ\",\"لَسْتُم\",\"لَسْتُمَا\",\"لَسْتُنَّ\",\"لَسْتِ\",\"لَسْنَ\",\"لَعَلَّ\",\"لَكِنَّ\",\"لَيْتَ\",\"لَيْسَ\",\"لَيْسَا\",\"لَيْسَتَا\",\"لَيْسَتْ\",\"لَيْسُوا\",\"لَِسْنَا\",\"ما\",\"ماانفك\",\"مابرح\",\"مادام\",\"ماذا\",\"مازال\",\"مافتئ\",\"مايو\",\"متى\",\"مثل\",\"مذ\",\"مساء\",\"مع\",\"معاذ\",\"مقابل\",\"مكانكم\",\"مكانكما\",\"مكانكنّ\",\"مكانَك\",\"مليار\",\"مليون\",\"مما\",\"ممن\",\"من\",\"منذ\",\"منها\",\"مه\",\"مهما\",\"مَنْ\",\"مِن\",\"نحن\",\"نحو\",\"نعم\",\"نفس\",\"نفسه\",\"نهاية\",\"نَخْ\",\"نِعِمّا\",\"نِعْمَ\",\"ها\",\"هاؤم\",\"هاكَ\",\"هاهنا\",\"هبّ\",\"هذا\",\"هذه\",\"هكذا\",\"هل\",\"هلمَّ\",\"هلّا\",\"هم\",\"هما\",\"هن\",\"هنا\",\"هناك\",\"هنالك\",\"هو\",\"هي\",\"هيا\",\"هيت\",\"هيّا\",\"هَؤلاء\",\"هَاتانِ\",\"هَاتَيْنِ\",\"هَاتِه\",\"هَاتِي\",\"هَجْ\",\"هَذا\",\"هَذانِ\",\"هَذَيْنِ\",\"هَذِه\",\"هَذِي\",\"هَيْهَاتَ\",\"و\",\"و6\",\"وا\",\"واحد\",\"واضاف\",\"واضافت\",\"واكد\",\"وان\",\"واهاً\",\"واوضح\",\"وراءَك\",\"وفي\",\"وقال\",\"وقالت\",\"وقد\",\"وقف\",\"وكان\",\"وكانت\",\"ولا\",\"ولم\",\"ومن\",\"مَن\",\"وهو\",\"وهي\",\"ويكأنّ\",\"وَيْ\",\"وُشْكَانََ\",\"يكون\",\"يمكن\",\"يوم\",\"كان\",\"ّأيّان\"}\n",
    "\n",
    "st = ISRIStemmer()\n",
    "def stemNLTK(text):\n",
    "    # tokenize\n",
    "    zen = TextBlob(text)\n",
    "    words = zen.words\n",
    "    cleaned = list()\n",
    "    for w in words:\n",
    "        ty = st.stem(w)\n",
    "        cleaned.append(ty)\n",
    "    return \" \".join(cleaned)\n",
    "\n",
    "\n",
    "def normalizeArabic(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    noise = re.compile(\"\"\" ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    text = re.sub(noise, '', text)\n",
    "    text = re.sub(r'(.)\\1+', r\"\\1\\1\", text) # Remove longation\n",
    "    return araby.strip_tashkeel(text)\n",
    "\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    zen = TextBlob(text)\n",
    "    words = zen.words\n",
    "    return \" \".join([w for w in words if not w in stops and not w in stop_word_comp and len(w) >= 2])\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    ## Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~\"\"\"), ' ', text)  # remove punctuation\n",
    "    ## remove extra whitespace\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    ## Convert text to lowercases\n",
    "    text = text.lower()\n",
    "    ## Remove stop words\n",
    "    text = remove_stop_words(text)\n",
    "    ## Remove numbers\n",
    "    text = re.sub(\"\\d+\", \" \", text)\n",
    "    ## Remove Tashkeel\n",
    "    text = normalizeArabic(text)\n",
    "    #text = re.sub('\\W+', ' ', text)\n",
    "    text = re.sub('[A-Za-z]+',' ',text)\n",
    "    text = re.sub(r'\\\\u[A-Za-z0-9\\\\]+',' ',text)\n",
    "    ## remove extra whitespace\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    #Stemming\n",
    "    text = stemNLTK(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee63b516",
   "metadata": {},
   "source": [
    "# Predict Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51e79444",
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_Len = 106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "964cdf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = torch.load('trans.pt')\n",
    "model1.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8788078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"UBC-NLP/MARBERT\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def predict(text):\n",
    "    text = clean_text(text)\n",
    "    encoded_review = tokenizer.encode_plus(text, max_length=Max_Len,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=False,\n",
    "    pad_to_max_length=True, #True,\n",
    "    truncation='longest_first',\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'\n",
    "    )\n",
    "    input_ids = encoded_review['input_ids'].to(device) #(input_ids + ([tokenizer.pad_token_id] * padding_length)).to(device)  \n",
    "    attention_mask = encoded_review['attention_mask'].to(device)\n",
    "    \n",
    "    output = model1(input_ids, attention_mask)\n",
    "    _, prediction = torch.max(output[0], dim=1)\n",
    "    return prediction[0].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d432b546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(predict(\"اقدر كمل سلب قصه اقل يقل عنه نفع ارو سلسل خلج ذاج\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5ef5f3",
   "metadata": {},
   "source": [
    "# Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "519650e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c84bc2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "@app.route('/predict/<string:x>', methods=['GET'])\n",
    "def predict1(x):\n",
    "    x = clean_text(x)\n",
    "    x = predict(x)\n",
    "    return str(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9c61ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5726/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [15/Mar/2023 03:30:07] \"GET /predict/كتاب%20جيد%20للغايه HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.run(port=5726)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
