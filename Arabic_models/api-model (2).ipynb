{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"app = Flask(__name__)\n@app.route('/predict/<string:sentence>', methods=['GET'])\ndef predict1(sentence):\n    # if this language of sentence is arabic\n    if detectText(sentence) =='ar':\n        # Load first model of arabic which represent Subjecitve or Objective class\n        model1 = torch.load('Arabic_Subjecitive_Objective.pt',map_location=torch.device('cpu'))\n        # Load Tokenizer for MARBERT transformer\n        tokenizer = AutoTokenizer.from_pretrained(\"UBC-NLP/MARBERT\")\n        # Make Auto Correction to arabic sentence\n        sentence_after_correction = corr.contextual_correct(sentence)\n        temp = sentence_after_correction\n        # Make  Arabic Cleaning process to sentence\n        sentence_after_correction_cleaned = clean_text_arabic(sentence_after_correction)\n        # Make predict (subjective - objective) (1 - 0)\n        result = predict(sentence_after_correction_cleaned,model1 , tokenizer , 106)\n        # Result is zero mean it Objective (Postive - Negative )\n        if str(result) =='0':\n            # Load second model which represent Positive or Negative\n            model2 = torch.load('Arabic_Postive_Negative.pt',map_location=torch.device('cpu'))\n            # Make Cleaning proecces to sentence\n            sentence_after_correction_cleaned = clean_text_arabic(temp)\n            # Make Predict (postive - negative)(1 - 0)\n            result = predict(sentence_after_correction_cleaned,model2,tokenizer, 106)\n            return str(result)\n        # Otherwise mean it Subective (Neutral)\n        else:\n            return '-1'\n    # this language of sentence is english\n    else:\n         # Load first model of english which represent Subjecitve or Objective class\n        model1 = torch.load('English_subjective_objective.pt',map_location=torch.device('cpu'))\n        # Load tokenizer for distilbert-base-uncased transformer\n        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n          # Make Auto Correction to english sentence\n        sentence_after_correction = spell(sentence)\n        temp = sentence_after_correction\n        # Make Engish Cleaning process to sentence\n        sentence_after_correction_cleaned = clean_text_english(sentence_after_correction)\n        # Make predict (subjective - objective) (1 - 0)\n        result = predict(sentence_after_correction_cleaned,model1 , tokenizer , 128)\n         # Result is zero mean it Objective (Postive - Negative )\n        if str(result) =='0':\n            # Load second model which represent Positive or Negative\n            model2 = torch.load('English_pos_neg.pt',map_location=torch.device('cpu'))\n            # Make Engish Cleaning proecces to sentence\n            sentence_after_correction_cleaned = clean_text_english(temp)\n             # Make Predict (postive - negative)(1 - 0)\n            result = predict(sentence_after_correction_cleaned,model2 , tokenizer , 128)\n            return str(result)\n        # Otherwise mean it Subective (Neutral)\n        else:\n            return '-1'\n        ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@app.route('/summaryText/<string:text>', methods=['GET'])\ndef summary_text(text):\n    # Represent name of summary model\n    model_name = \"pszemraj/led-large-book-summary\"\n    # Load Toknizer for summarization\n    tokenizer = LEDTokenizer.from_pretrained(model_name)\n    # The model is also initialized with the pre-trained model name defined above\n    model = LEDForConditionalGeneration.from_pretrained(model_name)\n    # The encoder method \n    inputs = tokenizer.encode(text, padding=\"max_length\", truncation=True, max_length=4096, return_tensors=\"pt\")\n    # Summary of encoded text using pretrained model\n    summary_ids = model.generate(inputs, max_length=512, num_beams=5, early_stopping=True)\n    # Decode the summary output using toknizer \n    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@app.route('/summaryYoutube/<string:video_id>', methods=['GET'])\ndef summary_youtube(video_id):\n    # Feach content of video with it's id\n    textParts = YouTubeTranscriptApi.get_transcript(video_id)\n    # Merge parts of texts togather\n    text = ' '.join([c['text'] for c in textParts])\n    # Represent name of summary model\n    model_name = \"pszemraj/led-large-book-summary\"\n    # Load Toknizer for summarization\n    tokenizer = LEDTokenizer.from_pretrained(model_name)\n    # The model is also initialized with the pre-trained model name defined above\n    model = LEDForConditionalGeneration.from_pretrained(model_name)\n    # The encoder method \n    inputs = tokenizer.encode(text, padding=\"max_length\", truncation=True, max_length=4096, return_tensors=\"pt\")\n    # Summary of encoded text using pretrained model\n    summary_ids = model.generate(inputs, max_length=512, num_beams=5, early_stopping=True)\n    # Decode the summary output using toknizer \n    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary","metadata":{},"execution_count":null,"outputs":[]}]}