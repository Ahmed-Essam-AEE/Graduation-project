{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e2871a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Text Preprocessing\n",
    "import re, string\n",
    "import nltk\n",
    "import scapy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "import pyarabic.araby as araby\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Pytorch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# Transformers\n",
    "import transformers\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
    "from transformers import Trainer , TrainingArguments\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import LEDTokenizer, LEDForConditionalGeneration\n",
    "#random\n",
    "import random\n",
    "seed_value=42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "#os and warnings\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "import pickle\n",
    "import requests\n",
    "import json\n",
    "from youtube_transcript_api import YouTubeTranscriptApi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc95e4e8",
   "metadata": {},
   "source": [
    "# Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c42b2bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"arabic\"))\n",
    "stop_word_comp = {\"،\",\"آض\",\"آمينَ\",\"آه\",\"آهاً\",\"آي\",\"أ\",\"أب\",\"أجل\",\"أجمع\",\"أخ\",\"أخذ\",\"أصبح\",\"أضحى\",\"أقبل\",\"أقل\",\"أكثر\",\"ألا\",\"أم\",\"أما\",\"أمامك\",\"أمامكَ\",\"أمسى\",\"أمّا\",\"أن\",\"أنا\",\"أنت\",\"أنتم\",\"أنتما\",\"أنتن\",\"أنتِ\",\"أنشأ\",\"أنّى\",\"أو\",\"أوشك\",\"أولئك\",\"أولئكم\",\"أولاء\",\"أولالك\",\"أوّهْ\",\"أي\",\"أيا\",\"أين\",\"أينما\",\"أيّ\",\"أَنَّ\",\"أََيُّ\",\"أُفٍّ\",\"إذ\",\"إذا\",\"إذاً\",\"إذما\",\"إذن\",\"إلى\",\"إليكم\",\"إليكما\",\"إليكنّ\",\"إليكَ\",\"إلَيْكَ\",\"إلّا\",\"إمّا\",\"إن\",\"إنّما\",\"إي\",\"إياك\",\"إياكم\",\"إياكما\",\"إياكن\",\"إيانا\",\"إياه\",\"إياها\",\"إياهم\",\"إياهما\",\"إياهن\",\"إياي\",\"إيهٍ\",\"إِنَّ\",\"ا\",\"ابتدأ\",\"اثر\",\"اجل\",\"احد\",\"اخرى\",\"اخلولق\",\"اذا\",\"اربعة\",\"ارتدّ\",\"استحال\",\"اطار\",\"اعادة\",\"اعلنت\",\"اف\",\"اكثر\",\"اكد\",\"الألاء\",\"الألى\",\"الا\",\"الاخيرة\",\"الان\",\"الاول\",\"الاولى\",\"التى\",\"التي\",\"الثاني\",\"الثانية\",\"الذاتي\",\"الذى\",\"الذي\",\"الذين\",\"السابق\",\"الف\",\"اللائي\",\"اللاتي\",\"اللتان\",\"اللتيا\",\"اللتين\",\"اللذان\",\"اللذين\",\"اللواتي\",\"الماضي\",\"المقبل\",\"الوقت\",\"الى\",\"اليوم\",\"اما\",\"امام\",\"امس\",\"ان\",\"انبرى\",\"انقلب\",\"انه\",\"انها\",\"او\",\"اول\",\"اي\",\"ايار\",\"ايام\",\"ايضا\",\"ب\",\"بات\",\"باسم\",\"بان\",\"بخٍ\",\"برس\",\"بسبب\",\"بسّ\",\"بشكل\",\"بضع\",\"بطآن\",\"بعد\",\"بعض\",\"بك\",\"بكم\",\"بكما\",\"بكن\",\"بل\",\"بلى\",\"بما\",\"بماذا\",\"بمن\",\"بن\",\"بنا\",\"به\",\"بها\",\"بي\",\"بيد\",\"بين\",\"بَسْ\",\"بَلْهَ\",\"بِئْسَ\",\"تانِ\",\"تانِك\",\"تبدّل\",\"تجاه\",\"تحوّل\",\"تلقاء\",\"تلك\",\"تلكم\",\"تلكما\",\"تم\",\"تينك\",\"تَيْنِ\",\"تِه\",\"تِي\",\"ثلاثة\",\"ثم\",\"ثمّ\",\"ثمّة\",\"ثُمَّ\",\"جعل\",\"جلل\",\"جميع\",\"جير\",\"حار\",\"حاشا\",\"حاليا\",\"حاي\",\"حتى\",\"حرى\",\"حسب\",\"حم\",\"حوالى\",\"حول\",\"حيث\",\"حيثما\",\"حين\",\"حيَّ\",\"حَبَّذَا\",\"حَتَّى\",\"حَذارِ\",\"خلا\",\"خلال\",\"دون\",\"دونك\",\"ذا\",\"ذات\",\"ذاك\",\"ذانك\",\"ذانِ\",\"ذلك\",\"ذلكم\",\"ذلكما\",\"ذلكن\",\"ذو\",\"ذوا\",\"ذواتا\",\"ذواتي\",\"ذيت\",\"ذينك\",\"ذَيْنِ\",\"ذِه\",\"ذِي\",\"راح\",\"رجع\",\"رويدك\",\"ريث\",\"رُبَّ\",\"زيارة\",\"سبحان\",\"سرعان\",\"سنة\",\"سنوات\",\"سوف\",\"سوى\",\"سَاءَ\",\"سَاءَمَا\",\"شبه\",\"شخصا\",\"شرع\",\"شَتَّانَ\",\"صار\",\"صباح\",\"صفر\",\"صهٍ\",\"صهْ\",\"ضد\",\"ضمن\",\"طاق\",\"طالما\",\"طفق\",\"طَق\",\"ظلّ\",\"عاد\",\"عام\",\"عاما\",\"عامة\",\"عدا\",\"عدة\",\"عدد\",\"عدم\",\"عسى\",\"عشر\",\"عشرة\",\"علق\",\"على\",\"عليك\",\"عليه\",\"عليها\",\"علًّ\",\"عن\",\"عند\",\"عندما\",\"عوض\",\"عين\",\"عَدَسْ\",\"عَمَّا\",\"غدا\",\"غير\",\"ـ\",\"ف\",\"فان\",\"فلان\",\"فو\",\"فى\",\"في\",\"فيم\",\"فيما\",\"فيه\",\"فيها\",\"قال\",\"قام\",\"قبل\",\"قد\",\"قطّ\",\"قلما\",\"قوة\",\"كأنّما\",\"كأين\",\"كأيّ\",\"كأيّن\",\"كاد\",\"كان\",\"كانت\",\"كذا\",\"كذلك\",\"كرب\",\"كل\",\"كلا\",\"كلاهما\",\"كلتا\",\"كلم\",\"كليكما\",\"كليهما\",\"كلّما\",\"كلَّا\",\"كم\",\"كما\",\"كي\",\"كيت\",\"كيف\",\"كيفما\",\"كَأَنَّ\",\"كِخ\",\"لئن\",\"لا\",\"لات\",\"لاسيما\",\"لدن\",\"لدى\",\"لعمر\",\"لقاء\",\"لك\",\"لكم\",\"لكما\",\"لكن\",\"لكنَّما\",\"لكي\",\"لكيلا\",\"للامم\",\"لم\",\"لما\",\"لمّا\",\"لن\",\"لنا\",\"له\",\"لها\",\"لو\",\"لوكالة\",\"لولا\",\"لوما\",\"لي\",\"لَسْتَ\",\"لَسْتُ\",\"لَسْتُم\",\"لَسْتُمَا\",\"لَسْتُنَّ\",\"لَسْتِ\",\"لَسْنَ\",\"لَعَلَّ\",\"لَكِنَّ\",\"لَيْتَ\",\"لَيْسَ\",\"لَيْسَا\",\"لَيْسَتَا\",\"لَيْسَتْ\",\"لَيْسُوا\",\"لَِسْنَا\",\"ما\",\"ماانفك\",\"مابرح\",\"مادام\",\"ماذا\",\"مازال\",\"مافتئ\",\"مايو\",\"متى\",\"مثل\",\"مذ\",\"مساء\",\"مع\",\"معاذ\",\"مقابل\",\"مكانكم\",\"مكانكما\",\"مكانكنّ\",\"مكانَك\",\"مليار\",\"مليون\",\"مما\",\"ممن\",\"من\",\"منذ\",\"منها\",\"مه\",\"مهما\",\"مَنْ\",\"مِن\",\"نحن\",\"نحو\",\"نعم\",\"نفس\",\"نفسه\",\"نهاية\",\"نَخْ\",\"نِعِمّا\",\"نِعْمَ\",\"ها\",\"هاؤم\",\"هاكَ\",\"هاهنا\",\"هبّ\",\"هذا\",\"هذه\",\"هكذا\",\"هل\",\"هلمَّ\",\"هلّا\",\"هم\",\"هما\",\"هن\",\"هنا\",\"هناك\",\"هنالك\",\"هو\",\"هي\",\"هيا\",\"هيت\",\"هيّا\",\"هَؤلاء\",\"هَاتانِ\",\"هَاتَيْنِ\",\"هَاتِه\",\"هَاتِي\",\"هَجْ\",\"هَذا\",\"هَذانِ\",\"هَذَيْنِ\",\"هَذِه\",\"هَذِي\",\"هَيْهَاتَ\",\"و\",\"و6\",\"وا\",\"واحد\",\"واضاف\",\"واضافت\",\"واكد\",\"وان\",\"واهاً\",\"واوضح\",\"وراءَك\",\"وفي\",\"وقال\",\"وقالت\",\"وقد\",\"وقف\",\"وكان\",\"وكانت\",\"ولا\",\"ولم\",\"ومن\",\"مَن\",\"وهو\",\"وهي\",\"ويكأنّ\",\"وَيْ\",\"وُشْكَانََ\",\"يكون\",\"يمكن\",\"يوم\",\"كان\",\"ّأيّان\"}\n",
    "\n",
    "st = ISRIStemmer()\n",
    "def stemNLTK(text):\n",
    "    # tokenize\n",
    "    zen = TextBlob(text)\n",
    "    words = zen.words\n",
    "    cleaned = list()\n",
    "    for w in words:\n",
    "        ty = st.stem(w)\n",
    "        cleaned.append(ty)\n",
    "    return \" \".join(cleaned)\n",
    "\n",
    "\n",
    "def normalizeArabic(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    noise = re.compile(\"\"\" ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "    text = re.sub(noise, '', text)\n",
    "    text = re.sub(r'(.)\\1+', r\"\\1\\1\", text) # Remove longation\n",
    "    return araby.strip_tashkeel(text)\n",
    "\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    zen = TextBlob(text)\n",
    "    words = zen.words\n",
    "    return \" \".join([w for w in words if not w in stops and not w in stop_word_comp and len(w) >= 2])\n",
    "\n",
    "\n",
    "\n",
    "def clean_text_arabic(text):\n",
    "    ## Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~\"\"\"), ' ', text)  # remove punctuation\n",
    "    ## remove extra whitespace\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    ## Convert text to lowercases\n",
    "    text = text.lower()\n",
    "    ## Remove stop words\n",
    "    text = remove_stop_words(text)\n",
    "    ## Remove numbers\n",
    "    text = re.sub(\"\\d+\", \" \", text)\n",
    "    ## Remove Tashkeel\n",
    "    text = normalizeArabic(text)\n",
    "    #text = re.sub('\\W+', ' ', text)\n",
    "    #text = re.sub('[A-Za-z]+',' ',text)\n",
    "    #text = re.sub(r'\\\\u[A-Za-z0-9\\\\]+',' ',text)\n",
    "    ## remove extra whitespace\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    #Stemming\n",
    "    text = stemNLTK(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31dac2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_english(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e78cb7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar\n",
      "en\n",
      "ar\n",
      "ar\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "def detectText(review):\n",
    "    lan = detect(review)\n",
    "    if lan == \"en\":\n",
    "        return \"en\"\n",
    "    else:\n",
    "        return \"ar\"\n",
    "\n",
    "print(detectText(\"كورس جميل جدا و الدكتور شرحه رائع و لكن data structure صعبة\"))\n",
    "print(detectText(\"good morning my name is Ahmed and i'm a student at القاهرة university\"))\n",
    "print(detectText(\"مستحيل impossible\"))\n",
    "print(detectText(\"اللغة العربية هي لغة القرأن\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa3064dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def predict(text,model1 , tokenizer , Max_len):\n",
    "    #text = clean_text(text)\n",
    "    encoded_review = tokenizer.encode_plus(text, max_length=Max_len,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=False,\n",
    "    pad_to_max_length=True, #True,\n",
    "    truncation='longest_first',\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'\n",
    "    )\n",
    "    input_ids = encoded_review['input_ids'].to(device) #(input_ids + ([tokenizer.pad_token_id] * padding_length)).to(device)  \n",
    "    attention_mask = encoded_review['attention_mask'].to(device)\n",
    "    \n",
    "    output = model1(input_ids, attention_mask)\n",
    "    _, prediction = torch.max(output[0], dim=1)\n",
    "    return prediction[0].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade9b29c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c612b4db",
   "metadata": {},
   "source": [
    "# Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed29bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "010a986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "@app.route('/predict/<string:x>', methods=['GET'])\n",
    "def predict1(x):\n",
    "    if detectText(x) =='ar':\n",
    "        model1 = torch.load('Arabic_Subjecitive_Objective.pt',map_location=torch.device('cpu'))\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"UBC-NLP/MARBERT\")\n",
    "        x2 = x\n",
    "        x = clean_text_arabic(x)\n",
    "        xx = predict(x,model1 , tokenizer , 106)\n",
    "        if str(xx) =='0':\n",
    "            model1 = torch.load('Arabic_Postive_Negative.pt',map_location=torch.device('cpu'))\n",
    "            x2 = clean_text_arabic(x2)\n",
    "            xx = predict(x2,model1,tokenizer, 106)\n",
    "            return str(xx)\n",
    "        else:\n",
    "            return '-1'\n",
    "    else:\n",
    "        model1 = torch.load('English_subjective_objective.pt',map_location=torch.device('cpu'))\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        x2 = x\n",
    "        x = clean_text_english(x)\n",
    "        xx = predict(x,model1 , tokenizer , 128)\n",
    "        if str(xx) =='0':\n",
    "            model1 = torch.load('English_pos_neg.pt',map_location=torch.device('cpu'))\n",
    "            x2 = clean_text_english(x2)\n",
    "            xx = predict(x2,model1 , tokenizer , 128)\n",
    "            return str(xx)\n",
    "        else:\n",
    "            return '-1'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1597773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/summaryText/<string:text>', methods=['GET'])\n",
    "def summary_text(text):\n",
    "    model_name = \"pszemraj/led-large-book-summary\"\n",
    "    tokenizer = LEDTokenizer.from_pretrained(model_name)\n",
    "    model = LEDForConditionalGeneration.from_pretrained(model_name)\n",
    "    inputs = tokenizer.encode(text, padding=\"max_length\", truncation=True, max_length=4096, return_tensors=\"pt\")\n",
    "    summary_ids = model.generate(inputs, max_length=512, num_beams=5, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99cebdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/summaryYoutube/<string:video_id>', methods=['GET'])\n",
    "def summary_youtube(video_id):\n",
    "    textParts = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "    text = ' '.join([c['text'] for c in textParts])\n",
    "    model_name = \"pszemraj/led-large-book-summary\"\n",
    "    tokenizer = LEDTokenizer.from_pretrained(model_name)\n",
    "    model = LEDForConditionalGeneration.from_pretrained(model_name)\n",
    "    inputs = tokenizer.encode(text, padding=\"max_length\", truncation=True, max_length=4096, return_tensors=\"pt\")\n",
    "    summary_ids = model.generate(inputs, max_length=512, num_beams=5, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c010273f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5726/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [25/Jun/2023 20:30:22] \"GET /predict/very%20boring%20book HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Jun/2023 20:30:39] \"GET /predict/كتاب%20وحش%20اوي%20منصحش%20بيه%20حد HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Jun/2023 20:30:46] \"GET /predict/كتاب%20وحش%20اوي%20منصحش%20بيه%20حد%20نهائي HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Jun/2023 20:30:52] \"GET /predict/كتاب%20حلو%20اوي HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Jun/2023 20:30:58] \"GET /predict/كتاب%20حلو%20اوي%20انصح%20بيه HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Jun/2023 20:31:07] \"GET /predict/كتاب%20جميل%20جدا HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Jun/2023 20:31:19] \"GET /predict/كتاب%20حلو%20جدا HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Jun/2023 20:31:27] \"GET /predict/كتاب%20حلو%20جدا%20انصحه%20بيه HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Jun/2023 20:31:36] \"GET /predict/كتاب%20حلو%20جدا%20انصحه%20بيه%20لأي%20حد HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [25/Jun/2023 20:31:48] \"GET /predict/كتاب%20سيء%20للغايه%20لا%20انصح%20بيه HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.run(port=5726)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7a8c88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
